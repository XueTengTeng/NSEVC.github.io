[{"title":"堆叠自编码机（SdA）","date":"2017-03-03T04:54:45.000Z","path":"2017/03/03/堆叠自编码机（SdA）/","text":"注意： 本节假设读者已经阅读了Classifying MNIST digits using Logistic Regression和Multilayer Perceptron。另外它使用了如下的Theano函数和概念：T.tanh, shared variables, basic arithmetic ops, T.grad, Random numbers, floatX。如果你要在GPU上跑代码，那么也需要阅读GPU。 注意： 这一节的代码可以在这里下载。 堆叠去噪自编码机（SdA）是堆叠自编码机展【Bengio07】的一个扩，并且在【Vincent08】加以介绍。 本教程是建立在之前Denoising Autoencoders的教程之上的。特别是如果你对自编码机没有任何概念的话，我们建议你先阅读上一章再进行深入地学习。 堆叠自编码机（SdA）去噪自编码机可以被堆叠成深度网络，通过对其增加隐藏表征（也就是code的输出——隐藏层的输出），隐藏层将下一层作为它的输入，再从这个输入中得到隐藏表征。对这样一个网络的结构，每次对一个层做无监督预训练（unsupervised pre-training）。每一层被当做是一个去噪编码机，通过极小化重构输入（这里的输入指的是上一层的code输出）误差来训练。一旦前k层训练了之后，我们可以训练第k+1层，因为我们可以从下层计算得到k+1层相应的code或者隐藏表征。 一旦所有的层都完成了预训练，那么网络将进行第二阶段的训练，被称为微调（fien-tuning）。这里我们考虑监督微调（supervised fine-tuning），我们都通一个监督任务希望极小化预测误差。为了达到这个目的，我们首先在网络的顶层之上（更确切的说是在输出层的输出code之上）再增加一个逻辑斯蒂回归层。接下来我们把整个网络当作是一个多层感知器（MLP）来训练。这时候，我们只要考虑每个自编码机的编码部分。这个阶段是有监督的，因为现在我们在训练过程中使用了类标签（关于多层感知器MLP更详细的内容见Multilayer Perceptron）。 我们可以很容易地通过theano这个框架来实现，使用我们在上一章去噪自编码机中定义的类。我们可以看到堆叠的去噪自编码机有两个facades：一个是一列自编码机，另一个是一个MLP。在预训练的时候，我们此采用第一个facade，独立地训练每一个自编码机。在第二阶段的训练中，我们采用第二个facade。这两个facades是相互关联的，因为： 自编码机和MLP的sigmoid层是共享参数的，并且 MLP中的中间层计算得到的隐藏表征，是作为自编码机的输入的。 1234567891011121314151617181920212223class SdA(object): def __init__( self, numpy_rng, theano_rng=None, n_ins=784, hidden_layers_sizes=[500, 500], n_outs=10, corruption_levels=[0.1, 0.1] ): self.sigmoid_layers = [] self.dA_layers = [] self.params = [] self.n_layers = len(hidden_layers_sizes) assert self.n_layers &gt; 0 if not theano_rng: theano_rng = RandomStreams(numpy_rng.randint(2 ** 30)) # allocate symbolic variables for the data self.x = T.matrix('x') # the data is presented as rasterized images self.y = T.ivector('y') # the labels are presented as 1D vector of self.sigmoid_layers会存储MLP的中间的sigmoid层，同时self.dA_layers会存储与MLP相关的去噪自编码层。 接下来，我们构造n_layers个sigmoid层和n_layers个去噪自编码机，这里的n_layers指的是我们的模型的深度。我们用MultilayerPerceptron中的HiddenLayer类来构造sigmoid层，但是会在HiddenLayer类的基础上做一个修改：我们将非线性的tanh函数替换成logistic函数\\(s(x)=\\frac{1}{1+e^{(-x)}}\\) 。我们将这些sigmoid层连接起来形成MLP，并且构造去噪自编码机，这样自编码机与相应的sigmoid层会共享权重矩阵和截距项。 1234567891011121314151617181920212223242526272829for i in range(self.n_layers): if i == 0: input_size = n_ins else: input_size = hidden_layers_sizes[i - 1] if i == 0: layer_input = self.x else: layer_input = self.sigmoid_layers[-1].output sigmoid_layer = HiddenLayer(rng=numpy_rng, input=layer_input, n_in=input_size, n_out=hidden_layers_sizes[i], activation=T.nnet.sigmoid) # add the layer to our list of layers self.sigmoid_layers.append(sigmoid_layer) self.params.extend(sigmoid_layer.params) dA_layer = dA(numpy_rng=numpy_rng, theano_rng=theano_rng, input=layer_input, n_visible=input_size, n_hidden=hidden_layers_sizes[i], W=sigmoid_layer.W, bhid=sigmoid_layer.b) self.dA_layers.append(dA_layer) 现在我们要做的就是在顶层的sigmoid层上增加一个逻辑斯蒂层，这样我们就可以得到一个MLP。我们将会使用在Classifying MNIST digits using Logistic Regression 中介绍的12345678910111213141516171819```python# We now need to add a logistic layer on top of the MLPself.logLayer = LogisticRegression( input=self.sigmoid_layers[-1].output, n_in=hidden_layers_sizes[-1], n_out=n_outs)self.params.extend(self.logLayer.params)# construct a function that implements one step of finetunining# compute the cost for second phase of training,# defined as the negative log likelihoodself.finetune_cost = self.logLayer.negative_log_likelihood(self.y)# compute the gradients with respect to the model parameters# symbolic variable that points to the number of errors made on the# minibatch given by self.x and self.yself.errors = self.logLayer.errors(self.y) SdA类也为去噪自编码机中的层提供了一个形成训练函数的方法。这些训练函数以列表的形式返回，这个列表中的元素i是一个函数，用来训练相应的层i的dA。 12def pretraining_functions(self, train_set_x, batch_size): index = T.lscalar('index') # index to a minibatch 为了使得在训练中输入的受损程度和学习率能够改变，我们将他们声明为Theano变量。 1234567891011121314151617181920212223242526272829corruption_level = T.scalar('corruption') # % of corruption to uselearning_rate = T.scalar('lr') # learning rate to use# begining of a batch, given `index`batch_begin = index * batch_size# ending of a batch given `index`batch_end = batch_begin + batch_sizepretrain_fns = []for dA in self.dA_layers: # get the cost and the updates list cost, updates = dA.get_cost_updates(corruption_level, learning_rate) # compile the theano function fn = theano.function( inputs=[ index, theano.In(corruption_level, value=0.2), theano.In(learning_rate, value=0.1) ], outputs=cost, updates=updates, givens=&#123; self.x: train_set_x[batch_begin: batch_end] &#125; ) # append `fn` to the list of functions pretrain_fns.append(fn)return pretrain_fns 现在pretrain_fns[i]中的任意一个函数都将index、corruption—输入受损程度和lr—学习率作为参数，且corruption参数是可选的。注意，参数的名字是由Theano变量在构造的时候给予的，而不是Python变量的名字（learning_rate或corruption_level）。当我们使用Theano的时候，要将这一点铭记于心。 我们以相同的方式为微调阶段构造需要的函数（train_fn，valid_score和test_score）。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374def build_finetune_functions(self, datasets, batch_size, learning_rate): (train_set_x, train_set_y) = datasets[0] (valid_set_x, valid_set_y) = datasets[1] (test_set_x, test_set_y) = datasets[2] # compute number of minibatches for training, validation and testing n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] n_valid_batches //= batch_size n_test_batches = test_set_x.get_value(borrow=True).shape[0] n_test_batches //= batch_size index = T.lscalar('index') # index to a [mini]batch # compute the gradients with respect to the model parameters gparams = T.grad(self.finetune_cost, self.params) # compute list of fine-tuning updates updates = [ (param, param - gparam * learning_rate) for param, gparam in zip(self.params, gparams) ] train_fn = theano.function( inputs=[index], outputs=self.finetune_cost, updates=updates, givens=&#123; self.x: train_set_x[ index * batch_size: (index + 1) * batch_size ], self.y: train_set_y[ index * batch_size: (index + 1) * batch_size ] &#125;, name='train' ) test_score_i = theano.function( [index], self.errors, givens=&#123; self.x: test_set_x[ index * batch_size: (index + 1) * batch_size ], self.y: test_set_y[ index * batch_size: (index + 1) * batch_size ] &#125;, name='test' ) valid_score_i = theano.function( [index], self.errors, givens=&#123; self.x: valid_set_x[ index * batch_size: (index + 1) * batch_size ], self.y: valid_set_y[ index * batch_size: (index + 1) * batch_size ] &#125;, name='valid' ) # Create a function that scans the entire validation set def valid_score(): return [valid_score_i(i) for i in range(n_valid_batches)] # Create a function that scans the entire test set def test_score(): return [test_score_i(i) for i in range(n_test_batches)] return train_fn, valid_score, test_score 注意valid_score和test_score不是Theano函数，而是Python函数，用来遍历整个验证集和整个测试集，然后分别对这两个集合产生各自的损失列表。 Putting it All Together下面的几行代码是来构造堆叠去噪自编码机的： 123456789numpy_rng = numpy.random.RandomState(89677)print('... building the model')# construct the stacked denoising autoencoder classsda = SdA( numpy_rng=numpy_rng, n_ins=28 * 28, hidden_layers_sizes=[1000, 1000, 1000], n_outs=10) 在训练这个网络的时候有两个阶段：逐层与训练，微调。 对于预训练阶段，我们将会遍历整个网络的所有层。对于每一个层，我们会用编辑好的Theano函数，在减少该层的重构误差使得权重最优化的SGD过程中使用。这个函数会在训练训练集的时候用到，并且训练固定次数，由pretraining_epochs参数给出。 123456789101112131415161718192021222324252627########################## PRETRAINING THE MODEL ##########################print('... getting the pretraining functions')pretraining_fns = sda.pretraining_functions(train_set_x=train_set_x, batch_size=batch_size)print('... pre-training the model')start_time = timeit.default_timer()## Pre-train layer-wisecorruption_levels = [.1, .2, .3]for i in range(sda.n_layers): # go through pretraining epochs for epoch in range(pretraining_epochs): # go through the training set c = [] for batch_index in range(n_train_batches): c.append(pretraining_fns[i](index=batch_index, corruption=corruption_levels[i], lr=pretrain_lr)) print('Pre-training layer %i, epoch %d, cost %f' % (i, epoch, numpy.mean(c, dtype='float64')))end_time = timeit.default_timer()print(('The pretraining code for file ' + os.path.split(__file__)[1] + ' ran for %.2fm' % ((end_time - start_time) / 60.)), file=sys.stderr) 微调的循环与Multilayer Perceptron中的循环非常相似。唯一的区别就是这个循环使用了build_finetune_functions这个函数产生的函数。 Running the Code读者可以通过如下命令运行代码： Python code/SdA.py 代码默认的是对每一层进行15期的预训练，同时batch size设为1。第一层的输入损失程度为0.1，第二层的输入损失程度为0.2，第三层的输入损失程度为0.3。预训练的学习率为0.001，微调阶段的学习率为0.1。预训练用了585.01分钟，对于每一期平均为13分钟。微调阶段花费了444.2分钟，运行了36期，平均每一期为12.34分钟。最后验证集的得分为1.39%，测试集的得分为1.3%。这些结果是在Intel Xeon E5430@2.66GHz CPU，单线程GotoBLASter的电脑上得到的。 技巧一个加快代码运行时间的方法是（假设你有足够的内存），计算到k-1层的网络是如何改变你的数据的。也就是说，我们从第一层dA开始训练，一旦这个训练完成之后，我们可以根据数据集中的数据点计算隐藏节点的值，并且将这些隐藏节点的值存储为一个新的数据集，这个数据集将会在第2层相应的dA训练中用到。一旦我们训练完了第2层相应的dA之后，我们可以以相同的方式计算得到另一个新的数据集供第3层相应的dA训练使用，如此往复。现在你可以看到，以这种方式，dAs都是独立地训练的，并且他们仅仅是对数据的一个非线性变化（从一个到另一个）。一旦所有的dAs都训练完成之后，我们就可以开始进行模型的微调。","tags":[{"name":"Theano","slug":"Theano","permalink":"http://yoursite.com/tags/Theano/"},{"name":"翻译","slug":"翻译","permalink":"http://yoursite.com/tags/翻译/"},{"name":"DL","slug":"DL","permalink":"http://yoursite.com/tags/DL/"},{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}]},{"title":"去噪自编码机（dA）","date":"2017-02-21T08:33:09.000Z","path":"2017/02/21/自编码机（dA）/","text":"注意： 本节假设读者已经阅读了Classifying MNIST digits using Logistic Regression和Multilayer Perceptron。另外它使用了如下的Theano函数和概念：T.tanh, shared variables, basic arithmetic ops, T.grad, Random numbers, floatX。如果你要在GPU上跑代码，那么也需要阅读GPU。 注意： 这一节的代码可以在这里下载。 降噪自编码机（dA）是经典的自编码机的一个扩展，并且在[Vincent08]中是以深度网络的组成成分来介绍的。 自编码机关于自编码机，我们可以在【bengio09】的4.6节中有一个总的概览。自编码机将\\(x\\in[0,1]^d\\)作为输入，并将输入映射（用一个编码机）到一个隐层的表征向量\\(y\\in[0,1]^{d’}\\)，中间特定的映射法则可以是如下的方式：$$y=s(Wx+b)$$ 这里的s是一个非线性的函数，例如sigmoid函数。这个隐层的表征向量y或者称为密码，接下来再映射成z（用一个解码机，z也陈给是重构的x），这里的z与原来的输入x是有相同的shape的。这里的映射与之前的映射也有着相似的形式，如下：$$z=s(W’y+b’)$$ （这里，W和b右上方的符号并不表示矩阵的转置）z应该看成是，在给定了密码y之后，对x的一个预测。当然，在逆映射中的权重矩阵\\(w’\\)可以被约束成\\(W’= W^{T}\\), \\(W’\\)为前一个映射中的权重矩阵的转置。这样被称为是tied weights。模型的参数（也就是W，b，b’, 当然如果不使用tied weights，还有W’）可以通过最小化平均重构误差来得到。 重构误差可以通过很多方式来衡量，取决于对给定了密码的输入的分布的合理假设（？？）。我们可以使用传统的均方误差\\(L(xz)=||x-z||^2\\) 。当然如果输入被处理成位向量（向量中的值均为0或1）或者概率位向量（向量中的值均为概率值），那么我们也可以用重构误差的交叉熵形式：$$L_{H}=-\\sum_{k=1}^{d} [x_{k}log(z_{k}+(1-x_{k})log(1-z_{k})]$$ 我们的希望是密码y，可以捕捉到数据集中主要因子变化的分布表示。这与在主成分上做投影的方式是十分相似的，会捕捉到数据中主要因子的变化。事实上，如果网络中，我们使用的是一个线性的隐藏层，并且我们将均方误差作为衡量损失的标准来训练这个网络，那么k个隐藏节点将学习反映数据中前k个主成分。如果隐藏层是非线性的，那么自编码机将于PCA表现不同，这样的网络将会学习捕获输入分布的多个方面。正是与PCA的不同，当我们考虑建立一个深度的自编码机的时候，考虑到堆叠多个编码机的时候，这样的不同将会变得非常重要【Hinton06】。 因为我们将y看做是x的有损压缩，所以对所有的x不可能都是一个好的压缩（有small-loss）。通过优化的方法，我们可以得到一个对训练样本而言较好的压缩，同时期望对其他的一些输入也是表现良好的，但是并不是对所有的输入都是表现良好的。自编码机概括起来就是：对于测试样本的分布与训练样本分布一样的测试集，自编码机得到一个较小的重构误差，但是对于在输入空间上随意选取的样本，往往有较高的重构误差。 我们用Theano框架来实现自编码机，以一个类的形式，这样子我们可以在构建堆叠自编码机的时候也能使用。首先第一步我们要为自编码机的参数W，b，b’,创建共享变量。（在本教程中，我们使用tied weights，也就是\\\\(W’=W^{T}\\)： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667def __init__( self, numpy_rng, theano_rng=None, input=None, n_visible=784, #可视节点数（相对于隐藏节点），也即输入输出的节点数 n_hidden=500, #隐藏节点数 W=None, bhid=None, bvis=None): self.n_visible = n_visible self.n_hidden = n_hidden if not theano_rng: #theano_rng == None的时候执行 theano_rng = RandomStreams(numpy_rng.randint(2 ** 30)) if not W: initial_W = numpy.asarray( numpy_rng.uniform( low=-4 * numpy.sqrt(6. / (n_hidden + n_visible)), high=4 * numpy.sqrt(6. / (n_hidden + n_visible)), size=(n_visible, n_hidden) ), dtype=theano.config.floatX ) W = theano.shared(value=initial_W, name='W', borrow=True) # b_prime的初始化，初始化为0 if not bvis: bvis = theano.shared( value=numpy.zeros( n_visible, dtype=theano.config.floatX ), borrow=True ) # b的初始化，初始化为0 if not bhid: bhid = theano.shared( value=numpy.zeros( n_hidden, dtype=theano.config.floatX ), name='b', borrow=True ) self.W = W # b corresponds to the bias of the hidden self.b = bhid # b_prime corresponds to the bias of the visible self.b_prime = bvis # tied weights, therefore W_prime is W transpose # 这里是以tied weights的形式 self.W_prime = self.W.T self.theano_rng = theano_rng if input is None: self.x = T.dmatrix(name='input') else: self.x = input # 整理模型的参数 self.params = [self.W, self.b, self.b_prime] 注意到我们在自编码机的input参数中，以符号的形式作为输入。这样可以使得我们在构建深度网络的时候，可以连接各个层：层k的符号形式的输出将会作为层k+1的符号形式的输入。 现在我们展示隐藏表征的计算和重构输入的计算：1234def get_hidden_values(self, input): return T.nnet.sigmoid(T.dot(input, self.W) + self.b)def get_reconstructed_input(self, hidden): return T.nnet.sigmoid(T.dot(hidden, self.W_prime) + self.b_prime) 使用上述的函数，我们就可以计算损失，并且得到随机梯度下降的更新步骤：123456789101112131415161718def get_cost_updates(self, corruption_level, learning_rate): tilde_x = self.get_corrupted_input(self.x, corruption_level) y = self.get_hidden_values(tilde_x) z = self.get_reconstructed_input(y) L = - T.sum(self.x * T.log(z) + (1 - self.x) * T.log(1 - z), axis=1) cost = T.mean(L) gparams = T.grad(cost, self.params) updates = [ (param, param - learning_rate * gparam) for param, gparam in zip(self.params, gparams) ] return (cost, updates) 我们可以定义一个函数，这个函数可以迭代地更新参数W，b和b_prime，这样我们得到的重构误差可以近似取得最小。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117da = dA( numpy_rng=rng, theano_rng=theano_rng, input=x, n_visible=28 * 28, n_hidden=500 ) cost, updates = da.get_cost_updates( corruption_level=0., learning_rate=learning_rate ) train_da = theano.function( [index], cost, updates=updates, givens=&#123; x: train_set_x[index * batch_size: (index + 1) * batch_size] &#125; ) start_time = timeit.default_timer() ############ # TRAINING # ############ # go through training epochs for epoch in range(training_epochs): # go through trainng set c = [] for batch_index in range(n_train_batches): c.append(train_da(batch_index)) print('Training epoch %d, cost ' % epoch, numpy.mean(c, dtype='float64')) end_time = timeit.default_timer() training_time = (end_time - start_time) print(('The no corruption code for file ' + os.path.split(__file__)[1] + ' ran for %.2fm' % ((training_time) / 60.)), file=sys.stderr) image = Image.fromarray( tile_raster_images(X=da.W.get_value(borrow=True).T, img_shape=(28, 28), tile_shape=(10, 10), tile_spacing=(1, 1))) image.save('filters_corruption_0.png') # start-snippet-3 ##################################### # BUILDING THE MODEL CORRUPTION 30% # ##################################### rng = numpy.random.RandomState(123) theano_rng = RandomStreams(rng.randint(2 ** 30)) da = dA( numpy_rng=rng, theano_rng=theano_rng, input=x, n_visible=28 * 28, n_hidden=500 ) cost, updates = da.get_cost_updates( corruption_level=0.3, learning_rate=learning_rate ) train_da = theano.function( [index], cost, updates=updates, givens=&#123; x: train_set_x[index * batch_size: (index + 1) * batch_size] &#125; ) start_time = timeit.default_timer() ############ # TRAINING # ############ # go through training epochs for epoch in range(training_epochs): # go through trainng set c = [] for batch_index in range(n_train_batches): c.append(train_da(batch_index)) print('Training epoch %d, cost ' % epoch, numpy.mean(c, dtype='float64')) end_time = timeit.default_timer() training_time = (end_time - start_time) print(('The 30% corruption code for file ' + os.path.split(__file__)[1] + ' ran for %.2fm' % (training_time / 60.)), file=sys.stderr) # end-snippet-3 # start-snippet-4 image = Image.fromarray(tile_raster_images( X=da.W.get_value(borrow=True).T, img_shape=(28, 28), tile_shape=(10, 10), tile_spacing=(1, 1))) image.save('filters_corruption_30.png') # end-snippet-4 os.chdir('../')if __name__ == '__main__':test_dA() 如果我们对最小化重构误差没有额外的约束，那么我们可能会希望我们的自编码机有n个输入，同时编码成n维（或者更大），以便学习到一个恒等函数，仅仅只是对输入做了一个拷贝。这样的一个自编码机是不能对来自不同输入分布的测试样本做任何区别的。 出人意料的是，在【Bengio07】的实验中表明，在实践中，当我们使用SGD、非线性的自编码机的时候，当将隐层节点的数量设置的比输入节点的数量多时，将会产生一些有用的表征（这里的有用，指的是网络将编码作为输入的时候，有较低的分类误差）。#这里表明我们需要将网络的隐藏节点设置的较多，并且采用非线性的网络结构的形式。 一个较为简单的解释是，随机梯度下降结合急躁停止策略等价于L2正则化。为了实现一个较好的对连续输入的重构，非线性的只有一个隐藏层的自编码机（就像上面的代码描述的那样），在第一层（编码层）的权重需要小一点，这样可以将隐藏节点的约束在线性区域，在第二层（解码层）的权重要较大。对于二进制的输入，在最小化重构误差的时候，也需要得到一个较大的权重。但是由于一些隐式的或是显示的正则化，使得我们很难得到一个大权重的解，最优化算法发现，编码只对于训练样本相似的样本表现的好，这正是我们想要的。这意味着representation提取的是在训练集中呈现的统计规则，而不是仅仅对输入进行复制。 自编码机隐藏层节点比输入节点多，也有其他的方式可以阻止学习恒等函数，并且在隐藏层的表示中捕获输入的有用信息。一种方式就是增加稀疏性（强制设置一些隐藏节点的值为0或者接近于0）。稀疏性在Ranzato07Lee08中已经被成功的采用。另外一种方法是在输入到输出的转化中增加一些随机性。这种技术在RBM（受限的玻尔兹曼机）中用到，在去噪自编码机中也用到，去噪自编码机将会在下面进行讨论。 去噪自编码机去噪自编码机背后的思想其实是非常简单的。为了使得隐藏层能够得到更鲁棒的特征并且防止它学习恒等函数，我们将会让自编码机从受损的输入来重构输入。 去噪自编码机是自编码机的随机版本。直觉上来说，一个去噪自编码机做了两件事：1.对输入进行编码（保留输入中蕴含的信息）2.让自编码机中输入随机受损的影响尽可能的降低。第二个只能通过捕获输入数据中的统计性依赖来完成。去噪自编码机也可以从不同的角度来理解（从流形学习的角度，随机操作的角度，信息自底向上流动的理论角度，自上而下的生成模型的角度），这些都在【Vincent08】中加以解释了。也可以在【Bengio09】的7.2节中查看自编码机的综述。在【Vincent08】中，随机地破坏输入数据的方式是，随机地将一些输入设置为0（几乎将近一半的数据）。因此，去噪自编码机是从不受损（比如：未遗失的）的数据中来预测受损（比如：遗失的）的数据，对于随机选的子集作为受损的数据。注意到，如何能够从剩余的未受损的数据中预测任意变量子集，对捕获一个变量集之间的联合分布是一个充分条件（这也是吉布斯采样的工作原理）。 为了将自编码机类转换成去噪自编码机类，我们只要对输入增加一个随机受损的操作。输入可以通过很多方式来达到受损的目的，但是在本教程中，我们仍然坚持最原始的受损机制，随机地将输入中的一些值设置为0。代码如下所示：1234def get_corrupted_input(self, input, corruption_level): return self.theano_rng.binomial(size=input.shape, n=1, p=1 - corruption_level, dtype=theano.config.floatX) * input 在堆叠自编码机类中，dA类的权重参数必须与相应的sigmoid层共享（？？）。因为这个原因，dA类的构造函数也会将Theano变量设置为共享变量。如果这些参数是None类型，那么将会重新构造一个新的类型。最终的去噪自编码机类如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596class dA(object): def __init__( self, numpy_rng, theano_rng=None, input=None, n_visible=784, #可视节点数（相对于隐藏节点），也即输入输出的节点数 n_hidden=500, #隐藏节点数 W=None, bhid=None, bvis=None ): self.n_visible = n_visible self.n_hidden = n_hidden if not theano_rng: #theano_rng == None的时候执行 theano_rng = RandomStreams(numpy_rng.randint(2 ** 30)) if not W: initial_W = numpy.asarray( numpy_rng.uniform( low=-4 * numpy.sqrt(6. / (n_hidden + n_visible)), high=4 * numpy.sqrt(6. / (n_hidden + n_visible)), size=(n_visible, n_hidden) ), dtype=theano.config.floatX ) W = theano.shared(value=initial_W, name='W', borrow=True) # b_prime的初始化，初始化为0 if not bvis: bvis = theano.shared( value=numpy.zeros( n_visible, dtype=theano.config.floatX ), borrow=True ) # b的初始化，初始化为0 if not bhid: bhid = theano.shared( value=numpy.zeros( n_hidden, dtype=theano.config.floatX ), name='b', borrow=True ) self.W = W # b corresponds to the bias of the hidden self.b = bhid # b_prime corresponds to the bias of the visible self.b_prime = bvis # tied weights, therefore W_prime is W transpose # 这里是以tied weights的形式 self.W_prime = self.W.T self.theano_rng = theano_rng if input is None: self.x = T.dmatrix(name='input') else: self.x = input # 整理模型的参数 self.params = [self.W, self.b, self.b_prime] def get_corrupted_input(self, input, corruption_level): return self.theano_rng.binomial(size=input.shape, n=1, p=1 - corruption_level, dtype=theano.config.floatX) * input def get_hidden_values(self, input): return T.nnet.sigmoid(T.dot(input, self.W) + self.b) def get_reconstructed_input(self, hidden): return T.nnet.sigmoid(T.dot(hidden, self.W_prime) + self.b_prime) def get_cost_updates(self, corruption_level, learning_rate): tilde_x = self.get_corrupted_input(self.x, corruption_level) y = self.get_hidden_values(tilde_x) z = self.get_reconstructed_input(y) L = - T.sum(self.x * T.log(z) + (1 - self.x) * T.log(1 - z), axis=1) cost = T.mean(L) gparams = T.grad(cost, self.params) updates = [ (param, param - learning_rate * gparam) for param, gparam in zip(self.params, gparams) ] return (cost, updates) Putting it All Together现在很容易构建一个dA类的实例并训练它。123# allocate symbolic variables for the dataindex = T.lscalar() # index to a [mini]batchx = T.matrix('x') # the data is presented as rasterized images 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051###################################### BUILDING THE MODEL CORRUPTION 30% ######################################rng = numpy.random.RandomState(123)theano_rng = RandomStreams(rng.randint(2 ** 30))da = dA( numpy_rng=rng, theano_rng=theano_rng, input=x, n_visible=28 * 28, n_hidden=500)cost, updates = da.get_cost_updates( corruption_level=0.3, learning_rate=learning_rate)train_da = theano.function( [index], cost, updates=updates, givens=&#123; x: train_set_x[index * batch_size: (index + 1) * batch_size] &#125;)start_time = timeit.default_timer()############# TRAINING ############## go through training epochsfor epoch in range(training_epochs): # go through trainng set c = [] for batch_index in range(n_train_batches): c.append(train_da(batch_index)) print('Training epoch %d, cost ' % epoch, numpy.mean(c, dtype='float64'))end_time = timeit.default_timer()training_time = (end_time - start_time)print(('The 30% corruption code for file ' + os.path.split(__file__)[1] + ' ran for %.2fm' % (training_time / 60.)), file=sys.stderr) 为了能够更好地感受网络学习到什么，我们将会绘制filter（权重矩阵）。记住，这个filter并不是全部，因为我们忽略了截距项，并且绘制的权重是经过与常数相乘后的（将权重转化到0-1之间）。 为了绘制filter我们需要借助tile_raster_images函数（Plotting Samples and Filters），我们希望读者能够学习相关内容。我们还需要使用Python的Image包，下面的几行代码是用来保存filter的图片的：12345image = Image.fromarray(tile_raster_images( X=da.W.get_value(borrow=True).T, img_shape=(28, 28), tile_shape=(10, 10), tile_spacing=(1, 1)))image.save('filters_corruption_30.png') Running the Code运行代码：Python dA.py 没有加任何噪声的filter如下所示： 加了30%噪声的filter如下所示：","tags":[{"name":"Theano","slug":"Theano","permalink":"http://yoursite.com/tags/Theano/"},{"name":"翻译","slug":"翻译","permalink":"http://yoursite.com/tags/翻译/"},{"name":"DL","slug":"DL","permalink":"http://yoursite.com/tags/DL/"},{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}]},{"title":"Hello World","date":"2017-02-21T06:33:02.908Z","path":"2017/02/21/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[]}]