[{"title":"自编码机（dA）","date":"2017-02-21T08:33:09.000Z","path":"2017/02/21/自编码机（dA）/","text":"注意： 本节假设读者已经阅读了Classifying MNIST digits using Logistic Regression和Multilayer Perceptron。另外它使用了如下的Theano函数和概念：T.tanh, shared variables, basic arithmetic ops, T.grad, Random numbers, floatX。如果你要在GPU上跑代码，那么也需要阅读GPU。 注意： 这一节的代码可以在这里下载。 降噪自编码机（dA）是经典的自编码机的一个扩展，并且在[Vincent08]中是以深度网络的组成成分来介绍的。 自编码机关于自编码机，我们可以在【bengio09】的4.6节中有一个总的概览。自编码机将\\(x\\in[0,1]^d\\)作为输入，并将输入映射（用一个编码机）到一个隐层的表征向量\\(y\\in[0,1]^{d’}\\)，中间特定的映射法则可以是如下的方式：$$y=s(Wx+b)$$ 这里的s是一个非线性的函数，例如sigmoid函数。这个隐层的表征向量y或者称为密码，接下来再映射成z（用一个解码机，z也陈给是重构的x），这里的z与原来的输入x是有相同的shape的。这里的映射与之前的映射也有着相似的形式，如下：$$z=s(W’y+b’)$$ （这里，W和b右上方的符号并不表示矩阵的转置）z应该看成是，在给定了密码y之后，对x的一个预测。当然，在逆映射中的权重矩阵\\(w’\\)可以被约束成\\(W’= W^{T}\\), \\(W’\\)为前一个映射中的权重矩阵的转置。这样被称为是tied weights。模型的参数（也就是W，b，b’, 当然如果不使用tied weights，还有W’）可以通过最小化平均重构误差来得到。重构误差可以通过很多方式来衡量，取决于对给定了密码的输入的分布的合理假设（？？）。我们可以使用传统的均方误差\\(L(xz)=||x-z||^2\\) 。当然如果输入被处理成位向量（向量中的值均为0或1）或者概率位向量（向量中的值均为概率值），那么我们也可以用重构误差的交叉熵形式：$$L_{H}=-\\sum_{k=1}^{d} [x_{k}log(z_{k}+(1-x_{k})log(1-z_{k})]$$ 我们的希望是密码y，可以捕捉到数据集中主要因子变化的分布表示。这与在主成分上做投影的方式是十分相似的，会捕捉到数据中主要因子的变化。事实上，如果网络中，我们使用的是一个线性的隐藏层，并且我们将均方误差作为衡量损失的标准来训练这个网络，那么k个隐藏节点将学习反映数据中前k个主成分。如果隐藏层是非线性的，那么自编码机将于PCA表现不同，这样的网络将会学习捕获输入分布的多个方面。正是与PCA的不同，当我们考虑建立一个深度的自编码机的时候，考虑到堆叠多个编码机的时候，这样的不同将会变得非常重要【Hinton06】。 因为我们将y看做是x的有损压缩，所以对所有的x不可能都是一个好的压缩（有small-loss）。通过优化的方法，我们可以得到一个对训练样本而言较好的压缩，同时期望对其他的一些输入也是表现良好的，但是并不是对所有的输入都是表现良好的。自编码机概括起来就是：对于测试样本的分布与训练样本分布一样的测试集，自编码机得到一个较小的重构误差，但是对于在输入空间上随意选取的样本，往往有较高的重构误差。 我们用Theano框架来实现自编码机，以一个类的形式，这样子我们可以在构建堆叠自编码机的时候也能使用。首先第一步我们要为自编码机的参数W，b，b’,创建共享变量。（在本教程中，我们使用tied weights，也就是\\\\(W’=W^{T}\\)： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667def __init__( self, numpy_rng, theano_rng=None, input=None, n_visible=784, #可视节点数（相对于隐藏节点），也即输入输出的节点数 n_hidden=500, #隐藏节点数 W=None, bhid=None, bvis=None): self.n_visible = n_visible self.n_hidden = n_hidden if not theano_rng: #theano_rng == None的时候执行 theano_rng = RandomStreams(numpy_rng.randint(2 ** 30)) if not W: initial_W = numpy.asarray( numpy_rng.uniform( low=-4 * numpy.sqrt(6. / (n_hidden + n_visible)), high=4 * numpy.sqrt(6. / (n_hidden + n_visible)), size=(n_visible, n_hidden) ), dtype=theano.config.floatX ) W = theano.shared(value=initial_W, name='W', borrow=True) # b_prime的初始化，初始化为0 if not bvis: bvis = theano.shared( value=numpy.zeros( n_visible, dtype=theano.config.floatX ), borrow=True ) # b的初始化，初始化为0 if not bhid: bhid = theano.shared( value=numpy.zeros( n_hidden, dtype=theano.config.floatX ), name='b', borrow=True ) self.W = W # b corresponds to the bias of the hidden self.b = bhid # b_prime corresponds to the bias of the visible self.b_prime = bvis # tied weights, therefore W_prime is W transpose # 这里是以tied weights的形式 self.W_prime = self.W.T self.theano_rng = theano_rng if input is None: self.x = T.dmatrix(name='input') else: self.x = input # 整理模型的参数 self.params = [self.W, self.b, self.b_prime] 注意到我们在自编码机的input参数中，以符号的形式作为输入。这样可以使得我们在构建深度网络的时候，可以连接各个层：层k的符号形式的输出将会作为层k+1的符号形式的输入。 现在我们展示隐藏表征的计算和重构输入的计算：1234def get_hidden_values(self, input): return T.nnet.sigmoid(T.dot(input, self.W) + self.b)def get_reconstructed_input(self, hidden): return T.nnet.sigmoid(T.dot(hidden, self.W_prime) + self.b_prime) 使用上述的函数，我们就可以计算损失，并且得到随机梯度下降的更新步骤：123456789101112131415161718def get_cost_updates(self, corruption_level, learning_rate): tilde_x = self.get_corrupted_input(self.x, corruption_level) y = self.get_hidden_values(tilde_x) z = self.get_reconstructed_input(y) L = - T.sum(self.x * T.log(z) + (1 - self.x) * T.log(1 - z), axis=1) cost = T.mean(L) gparams = T.grad(cost, self.params) updates = [ (param, param - learning_rate * gparam) for param, gparam in zip(self.params, gparams) ] return (cost, updates) 我们可以定义一个函数，这个函数可以迭代地更新参数W，b和b_prime，这样我们得到的重构误差可以近似取得最小。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117da = dA( numpy_rng=rng, theano_rng=theano_rng, input=x, n_visible=28 * 28, n_hidden=500 ) cost, updates = da.get_cost_updates( corruption_level=0., learning_rate=learning_rate ) train_da = theano.function( [index], cost, updates=updates, givens=&#123; x: train_set_x[index * batch_size: (index + 1) * batch_size] &#125; ) start_time = timeit.default_timer() ############ # TRAINING # ############ # go through training epochs for epoch in range(training_epochs): # go through trainng set c = [] for batch_index in range(n_train_batches): c.append(train_da(batch_index)) print('Training epoch %d, cost ' % epoch, numpy.mean(c, dtype='float64')) end_time = timeit.default_timer() training_time = (end_time - start_time) print(('The no corruption code for file ' + os.path.split(__file__)[1] + ' ran for %.2fm' % ((training_time) / 60.)), file=sys.stderr) image = Image.fromarray( tile_raster_images(X=da.W.get_value(borrow=True).T, img_shape=(28, 28), tile_shape=(10, 10), tile_spacing=(1, 1))) image.save('filters_corruption_0.png') # start-snippet-3 ##################################### # BUILDING THE MODEL CORRUPTION 30% # ##################################### rng = numpy.random.RandomState(123) theano_rng = RandomStreams(rng.randint(2 ** 30)) da = dA( numpy_rng=rng, theano_rng=theano_rng, input=x, n_visible=28 * 28, n_hidden=500 ) cost, updates = da.get_cost_updates( corruption_level=0.3, learning_rate=learning_rate ) train_da = theano.function( [index], cost, updates=updates, givens=&#123; x: train_set_x[index * batch_size: (index + 1) * batch_size] &#125; ) start_time = timeit.default_timer() ############ # TRAINING # ############ # go through training epochs for epoch in range(training_epochs): # go through trainng set c = [] for batch_index in range(n_train_batches): c.append(train_da(batch_index)) print('Training epoch %d, cost ' % epoch, numpy.mean(c, dtype='float64')) end_time = timeit.default_timer() training_time = (end_time - start_time) print(('The 30% corruption code for file ' + os.path.split(__file__)[1] + ' ran for %.2fm' % (training_time / 60.)), file=sys.stderr) # end-snippet-3 # start-snippet-4 image = Image.fromarray(tile_raster_images( X=da.W.get_value(borrow=True).T, img_shape=(28, 28), tile_shape=(10, 10), tile_spacing=(1, 1))) image.save('filters_corruption_30.png') # end-snippet-4 os.chdir('../')if __name__ == '__main__':test_dA() 如果我们对最小化重构误差没有额外的约束，那么我们可能会希望我们的自编码机有n个输入，同时编码成n维（或者更大），以便学习到一个恒等函数，仅仅只是对输入做了一个拷贝。这样的一个自编码机是不能对来自不同输入分布的测试样本做任何区别的。 出人意料的是，在【Bengio07】的实验中表明，在实践中，当我们使用SGD、非线性的自编码机的时候，当将隐层节点的数量设置的比输入节点的数量多时，将会产生一些有用的表征（这里的有用，指的是网络将编码作为输入的时候，有较低的分类误差）。#这里表明我们需要将网络的隐藏节点设置的较多，并且采用非线性的网络结构的形式。 一个较为简单的解释是，随机梯度下降结合急躁停止策略等价于L2正则化。为了实现一个较好的对连续输入的重构，非线性的只有一个隐藏层的自编码机（就像上面的代码描述的那样），在第一层（编码层）的权重需要小一点，这样可以将隐藏节点的约束在线性区域，在第二层（解码层）的权重要较大。对于二进制的输入，在最小化重构误差的时候，也需要得到一个较大的权重。但是由于一些隐式的或是显示的正则化，使得我们很难得到一个大权重的解，最优化算法发现，编码只对于训练样本相似的样本表现的好，这正是我们想要的。这意味着representation提取的是在训练集中呈现的统计规则，而不是仅仅对输入进行复制。 自编码机隐藏层节点比输入节点多，也有其他的方式可以阻止学习恒等函数，并且在隐藏层的表示中捕获输入的有用信息。一种方式就是增加稀疏性（强制设置一些隐藏节点的值为0或者接近于0）。稀疏性在Ranzato07Lee08中已经被成功的采用。另外一种方法是在输入到输出的转化中增加一些随机性。这种技术在RBM（受限的玻尔兹曼机）中用到，在去噪自编码机中也用到，去噪自编码机将会在下面进行讨论。 去噪自编码机去噪自编码机背后的思想其实是非常简单的。为了使得隐藏层能够得到更鲁棒的特征并且防止它学习恒等函数，我们将会让自编码机从受损的输入来重构输入。 去噪自编码机是自编码机的随机版本。直觉上来说，一个去噪自编码机做了两件事：1.对输入进行编码（保留输入中蕴含的信息）2.让自编码机中输入随机受损的影响尽可能的降低。第二个只能通过捕获输入数据中的统计性依赖来完成。去噪自编码机也可以从不同的角度来理解（从流形学习的角度，随机操作的角度，信息自底向上流动的理论角度，自上而下的生成模型的角度），这些都在【Vincent08】中加以解释了。也可以在【Bengio09】的7.2节中查看自编码机的综述。在【Vincent08】中，随机地破坏输入数据的方式是，随机地将一些输入设置为0（几乎将近一半的数据）。因此，去噪自编码机是从不受损（比如：未遗失的）的数据中来预测受损（比如：遗失的）的数据，对于随机选的子集作为受损的数据。注意到，如何能够从剩余的未受损的数据中预测任意变量子集，对捕获一个变量集之间的联合分布是一个充分条件（这也是吉布斯采样的工作原理）。 为了将自编码机类转换成去噪自编码机类，我们只要对输入增加一个随机受损的操作。输入可以通过很多方式来达到受损的目的，但是在本教程中，我们仍然坚持最原始的受损机制，随机地将输入中的一些值设置为0。代码如下所示：1234def get_corrupted_input(self, input, corruption_level): return self.theano_rng.binomial(size=input.shape, n=1, p=1 - corruption_level, dtype=theano.config.floatX) * input 在堆叠自编码机类中，1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798最终的去噪自编码机类如下： ```pythonclass dA(object): def __init__( self, numpy_rng, theano_rng=None, input=None, n_visible=784, #可视节点数（相对于隐藏节点），也即输入输出的节点数 n_hidden=500, #隐藏节点数 W=None, bhid=None, bvis=None ): self.n_visible = n_visible self.n_hidden = n_hidden if not theano_rng: #theano_rng == None的时候执行 theano_rng = RandomStreams(numpy_rng.randint(2 ** 30)) if not W: initial_W = numpy.asarray( numpy_rng.uniform( low=-4 * numpy.sqrt(6. / (n_hidden + n_visible)), high=4 * numpy.sqrt(6. / (n_hidden + n_visible)), size=(n_visible, n_hidden) ), dtype=theano.config.floatX ) W = theano.shared(value=initial_W, name=&apos;W&apos;, borrow=True) # b_prime的初始化，初始化为0 if not bvis: bvis = theano.shared( value=numpy.zeros( n_visible, dtype=theano.config.floatX ), borrow=True ) # b的初始化，初始化为0 if not bhid: bhid = theano.shared( value=numpy.zeros( n_hidden, dtype=theano.config.floatX ), name=&apos;b&apos;, borrow=True ) self.W = W # b corresponds to the bias of the hidden self.b = bhid # b_prime corresponds to the bias of the visible self.b_prime = bvis # tied weights, therefore W_prime is W transpose # 这里是以tied weights的形式 self.W_prime = self.W.T self.theano_rng = theano_rng if input is None: self.x = T.dmatrix(name=&apos;input&apos;) else: self.x = input # 整理模型的参数 self.params = [self.W, self.b, self.b_prime] def get_corrupted_input(self, input, corruption_level): return self.theano_rng.binomial(size=input.shape, n=1, p=1 - corruption_level, dtype=theano.config.floatX) * input def get_hidden_values(self, input): return T.nnet.sigmoid(T.dot(input, self.W) + self.b) def get_reconstructed_input(self, hidden): return T.nnet.sigmoid(T.dot(hidden, self.W_prime) + self.b_prime) def get_cost_updates(self, corruption_level, learning_rate): tilde_x = self.get_corrupted_input(self.x, corruption_level) y = self.get_hidden_values(tilde_x) z = self.get_reconstructed_input(y) L = - T.sum(self.x * T.log(z) + (1 - self.x) * T.log(1 - z), axis=1) cost = T.mean(L) gparams = T.grad(cost, self.params) updates = [ (param, param - learning_rate * gparam) for param, gparam in zip(self.params, gparams) ] return (cost, updates) Putting it All Together现在很容易构建一个dA类的实例并训练它。123# allocate symbolic variables for the dataindex = T.lscalar() # index to a [mini]batchx = T.matrix('x') # the data is presented as rasterized images 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051###################################### BUILDING THE MODEL CORRUPTION 30% ######################################rng = numpy.random.RandomState(123)theano_rng = RandomStreams(rng.randint(2 ** 30))da = dA( numpy_rng=rng, theano_rng=theano_rng, input=x, n_visible=28 * 28, n_hidden=500)cost, updates = da.get_cost_updates( corruption_level=0.3, learning_rate=learning_rate)train_da = theano.function( [index], cost, updates=updates, givens=&#123; x: train_set_x[index * batch_size: (index + 1) * batch_size] &#125;)start_time = timeit.default_timer()############# TRAINING ############## go through training epochsfor epoch in range(training_epochs): # go through trainng set c = [] for batch_index in range(n_train_batches): c.append(train_da(batch_index)) print('Training epoch %d, cost ' % epoch, numpy.mean(c, dtype='float64'))end_time = timeit.default_timer()training_time = (end_time - start_time)print(('The 30% corruption code for file ' + os.path.split(__file__)[1] + ' ran for %.2fm' % (training_time / 60.)), file=sys.stderr) 为了能够更好地感受网络学习到什么，我们将会绘制filter（权重矩阵）。记住，这个filter并不是全部，因为我们忽略了截距项，并且绘制的权重是经过与常数相乘后的（将权重转化到0-1之间）。 为了绘制filter我们需要借助tile_raster_images函数（Plotting Samples and Filters），我们希望读者能够学习相关内容。我们还需要使用Python的Image包，下面的几行代码是用来保存filter的图片的：12345image = Image.fromarray(tile_raster_images( X=da.W.get_value(borrow=True).T, img_shape=(28, 28), tile_shape=(10, 10), tile_spacing=(1, 1)))image.save('filters_corruption_30.png') Running the Code运行代码：Python dA.py没有加任何噪声的filter如下所示： 加了30%噪声的filter如下所示：","tags":[{"name":"Theano","slug":"Theano","permalink":"http://yoursite.com/tags/Theano/"},{"name":"翻译","slug":"翻译","permalink":"http://yoursite.com/tags/翻译/"},{"name":"DL","slug":"DL","permalink":"http://yoursite.com/tags/DL/"},{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}]},{"title":"Hello World","date":"2017-02-21T06:33:02.908Z","path":"2017/02/21/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[]}]